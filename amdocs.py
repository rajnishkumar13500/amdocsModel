# -*- coding: utf-8 -*-
"""Amdocs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v38i998SVxMFFunn7GyTp9RQHvqhefw_
"""

pip install xgboost==1.6.2 scikit-learn==1.1.3

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from xgboost import XGBRegressor
import lightgbm as lgb
import ast
import json

class CourseRecommender:
    def __init__(self):
        self.label_encoders = {}
        self.model = None
        self.feature_importance = None
        self.scaler = StandardScaler()

    def parse_course_history(self, course_history_str):
        """Parse course history string back into a list of dictionaries"""
        try:
            # First try to parse as a regular string representation of a list
            if isinstance(course_history_str, str):
                # Replace single quotes with double quotes for valid JSON
                course_history_str = course_history_str.replace("'", '"')
                try:
                    return json.loads(course_history_str)
                except json.JSONDecodeError:
                    try:
                        return ast.literal_eval(course_history_str)
                    except:
                        return []
            return course_history_str if isinstance(course_history_str, list) else []
        except:
            return []

    def preprocess_data(self, df):
        """Preprocess the dataset for training"""
        # Create copy to avoid modifying original data
        data = df.copy()

        # Parse course history and extract features
        data['course_history'] = data['course_history'].apply(self.parse_course_history)

        # Calculate average completion rate and duration
        data['avg_course_completion'] = data['course_history'].apply(
            lambda x: np.mean([float(course.get('completion_rate', 0)) for course in x]) if x else 0
        )
        data['avg_course_duration'] = data['course_history'].apply(
            lambda x: np.mean([float(course.get('duration_days', 0)) for course in x]) if x else 0
        )
        categorical_features = ['education_level', 'career_goal', 'preferred_learning_style']
        for feature in categorical_features:
          if feature not in self.label_encoders:
            self.label_encoders[feature] = LabelEncoder()
            # Fit on the entire dataset (df)
            self.label_encoders[feature].fit(df[feature])
          data[feature] = self.label_encoders[feature].transform(data[feature])

        # Select features for training
        selected_features = [
            'age', 'education_level', 'career_goal', 'preferred_learning_style',
            'time_availability_hours_per_week', 'learning_pace', 'weekly_study_hours',
            'platform_visits_per_week', 'engagement_score', 'avg_course_completion',
            'avg_course_duration'
        ]

        # Add skill columns
        skill_columns = [col for col in data.columns if col.startswith('skill_')]
        selected_features.extend(skill_columns)

        return data[selected_features]

    def train_model(self, df):
        """Train the recommendation model"""
        try:
            # Preprocess data
            X = self.preprocess_data(df)
            y = df['course_success_rate']

            # Split data
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

            # Scale features
            X_train_scaled = self.scaler.fit_transform(X_train)
            X_test_scaled = self.scaler.transform(X_test)

            # Train multiple models and compare
            models = {
                'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
                'XGBoost': XGBRegressor(random_state=42),
                'LightGBM': lgb.LGBMRegressor(random_state=42),
                'Gradient Boosting': GradientBoostingRegressor(random_state=42)
            }

            best_score = 0
            best_model = None
            results = {}

            for name, model in models.items():
                print(f"\nTraining {name}...")
                # Train model
                model.fit(X_train_scaled, y_train)

                # Make predictions
                y_pred = model.predict(X_test_scaled)

                # Calculate metrics
                r2 = r2_score(y_test, y_pred)
                mse = mean_squared_error(y_test, y_pred)
                mae = mean_absolute_error(y_test, y_pred)

                # Perform cross-validation
                cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)

                results[name] = {
                    'R2 Score': r2,
                    'MSE': mse,
                    'MAE': mae,
                    'CV Mean': cv_scores.mean(),
                    'CV Std': cv_scores.std()
                }

                print(f"{name} R2 Score: {r2:.4f}")

                # Update best model
                if r2 > best_score:
                    best_score = r2
                    best_model = model

                    # Store feature importance for best model
                    if hasattr(model, 'feature_importances_'):
                        self.feature_importance = pd.DataFrame({
                            'feature': X.columns,
                            'importance': model.feature_importances_
                        }).sort_values('importance', ascending=False)

            self.model = best_model
            return results

        except Exception as e:
            print(f"Error during training: {str(e)}")
            raise

# Example usage
if __name__ == "__main__":
    try:
        # Load the dataset
        print("Loading dataset...")
        df = pd.read_csv('adaptive_learning_dataset.csv')

        # Initialize and train the recommender
        print("Initializing recommender...")
        recommender = CourseRecommender()

        print("Training models...")
        results = recommender.train_model(df)

        # Print model comparison results
        print("\nModel Comparison Results:")
        for model_name, metrics in results.items():
            print(f"\n{model_name}:")
            for metric_name, value in metrics.items():
                print(f"{metric_name}: {value:.4f}")

        # Print feature importance for best model
        if recommender.feature_importance is not None:
            print("\nTop 10 Most Important Features:")
            print(recommender.feature_importance.head(10))

    except Exception as e:
        print(f"An error occurred: {str(e)}")

# prompt: generate code for visualisation of the above

import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'recommender' and 'results' are defined from the previous code execution
# and that feature_importance is not None

if recommender.feature_importance is not None:
    plt.figure(figsize=(10, 6))
    sns.barplot(x='importance', y='feature', data=recommender.feature_importance.head(10))
    plt.title('Top 10 Feature Importance')
    plt.xlabel('Importance')
    plt.ylabel('Feature')
    plt.show()


# Visualize model performance
model_names = list(results.keys())
r2_scores = [results[model]['R2 Score'] for model in model_names]
mse_scores = [results[model]['MSE'] for model in model_names]

plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.bar(model_names, r2_scores)
plt.title('R2 Scores of Different Models')
plt.xlabel('Model')
plt.ylabel('R2 Score')
plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability


plt.subplot(1, 2, 2)
plt.bar(model_names, mse_scores)
plt.title('MSE Scores of Different Models')
plt.xlabel('Model')
plt.ylabel('MSE')
plt.xticks(rotation=45, ha='right') # Rotate x-axis labels

plt.tight_layout()
plt.show()

# prompt: write code to print head of the data we importrd

# Assuming 'df' is the DataFrame loaded from 'adaptive_learning_dataset.csv'
print(df.head())



class CourseRecommender:
    def __init__(self):
        self.label_encoders = {}
        self.model = None
        self.feature_importance = None
        self.scaler = StandardScaler()
        self.is_trained = False  # Add flag to track if model is trained

    def parse_course_history(self, course_history_str):
        """Parse course history string back into a list of dictionaries"""
        try:
            if isinstance(course_history_str, str):
                course_history_str = course_history_str.replace("'", '"')
                try:
                    return json.loads(course_history_str)
                except json.JSONDecodeError:
                    try:
                        return ast.literal_eval(course_history_str)
                    except:
                        return []
            return course_history_str if isinstance(course_history_str, list) else []
        except:
            return []

    def preprocess_data(self, df, training=True):
        """Preprocess the dataset for training or prediction"""
        data = df.copy()

        # Ensure all required columns exist
        required_columns = [
            'age', 'education_level', 'career_goal', 'preferred_learning_style',
            'time_availability_hours_per_week', 'learning_pace', 'weekly_study_hours',
            'platform_visits_per_week', 'engagement_score', 'course_history'
        ]
        skills = ['python', 'statistics', 'machine_learning', 'html_css', 'javascript', 'react']
        for skill in skills:
            required_columns.append(f'skill_{skill}')

        for col in required_columns:
            if col not in data.columns:
                raise ValueError(f"Missing required column: {col}")

        # Parse course history
        data['course_history'] = data['course_history'].apply(self.parse_course_history)

        # Calculate course history features
        data['avg_course_completion'] = data['course_history'].apply(
            lambda x: np.mean([float(course.get('completion_rate', 0)) for course in x]) if x else 0
        )
        data['avg_course_duration'] = data['course_history'].apply(
            lambda x: np.mean([float(course.get('duration_days', 0)) for course in x]) if x else 0
        )

        # Handle categorical features
        categorical_features = ['education_level', 'career_goal', 'preferred_learning_style']
        for feature in categorical_features:
            if training:
                if feature not in self.label_encoders:
                    self.label_encoders[feature] = LabelEncoder()
                    self.label_encoders[feature].fit(data[feature])
            data[feature] = self.label_encoders[feature].transform(data[feature])

        # Select features
        selected_features = [
            'age', 'education_level', 'career_goal', 'preferred_learning_style',
            'time_availability_hours_per_week', 'learning_pace', 'weekly_study_hours',
            'platform_visits_per_week', 'engagement_score', 'avg_course_completion',
            'avg_course_duration'
        ]
        selected_features.extend([f'skill_{skill}' for skill in skills])

        return data[selected_features]

    def train_model(self, df):
        """Train the recommendation model"""
        try:
            X = self.preprocess_data(df, training=True)
            y = df['course_success_rate']

            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
            X_train_scaled = self.scaler.fit_transform(X_train)
            X_test_scaled = self.scaler.transform(X_test)

            # Use Random Forest as the default model for simplicity and reliability
            self.model = RandomForestRegressor(n_estimators=100, random_state=42)
            self.model.fit(X_train_scaled, y_train)

            # Store feature importance
            self.feature_importance = pd.DataFrame({
                'feature': X.columns,
                'importance': self.model.feature_importances_
            }).sort_values('importance', ascending=False)

            # Set trained flag
            self.is_trained = True

            # Calculate and return performance metrics
            y_pred = self.model.predict(X_test_scaled)
            return {
                'r2_score': r2_score(y_test, y_pred),
                'mse': mean_squared_error(y_test, y_pred),
                'mae': mean_absolute_error(y_test, y_pred)
            }

        except Exception as e:
            print(f"Error during training: {str(e)}")
            raise

    def recommend_courses(self, user_data):
        """Generate course recommendations for a user"""
        if not self.is_trained:
            raise ValueError("Model has not been trained yet. Please train the model first.")

        try:
            # Convert user data to DataFrame
            user_df = pd.DataFrame([user_data])

            # Preprocess user data
            user_features = self.preprocess_data(user_df, training=False)

            # Scale features
            user_features_scaled = self.scaler.transform(user_features)

            # Predict success rate
            predicted_success = self.model.predict(user_features_scaled)[0]

            # Generate recommendations based on user profile
            career_goal = user_data['career_goal']
            skill_levels = {k: v for k, v in user_data.items() if k.startswith('skill_')}

            recommended_courses = []
            skill_gaps = []

            # Recommendation logic for Data Science path
            if career_goal == 'Data Science':
                if skill_levels.get('skill_python', 0) < 60:
                    recommended_courses.append('Python Programming Fundamentals')
                    skill_gaps.append({
                        'skill': 'python',
                        'current_level': skill_levels.get('skill_python', 0),
                        'difficulty': 'beginner'
                    })
                if skill_levels.get('skill_statistics', 0) < 60:
                    recommended_courses.append('Statistics for Data Science')
                    skill_gaps.append({
                        'skill': 'statistics',
                        'current_level': skill_levels.get('skill_statistics', 0),
                        'difficulty': 'intermediate'
                    })
                if skill_levels.get('skill_machine_learning', 0) < 60:
                    recommended_courses.append('Machine Learning Basics')
                    skill_gaps.append({
                        'skill': 'machine_learning',
                        'current_level': skill_levels.get('skill_machine_learning', 0),
                        'difficulty': 'advanced'
                    })

            # Recommendation logic for Web Development path
            elif career_goal == 'Web Development':
                if skill_levels.get('skill_html_css', 0) < 60:
                    recommended_courses.append('HTML and CSS Fundamentals')
                    skill_gaps.append({
                        'skill': 'html_css',
                        'current_level': skill_levels.get('skill_html_css', 0),
                        'difficulty': 'beginner'
                    })
                if skill_levels.get('skill_javascript', 0) < 60:
                    recommended_courses.append('JavaScript Essentials')
                    skill_gaps.append({
                        'skill': 'javascript',
                        'current_level': skill_levels.get('skill_javascript', 0),
                        'difficulty': 'intermediate'
                    })
                if skill_levels.get('skill_react', 0) < 60:
                    recommended_courses.append('React.js Development')
                    skill_gaps.append({
                        'skill': 'react',
                        'current_level': skill_levels.get('skill_react', 0),
                        'difficulty': 'advanced'
                    })

            return {
                'predicted_success_rate': predicted_success,
                'recommended_courses': recommended_courses[:3],
                'skill_gaps': skill_gaps
            }

        except Exception as e:
            print(f"Error generating recommendations: {str(e)}")
            raise



if __name__ == "__main__":
    try:
        # Create sample training data
        print("Creating sample training data...")
        sample_data = pd.DataFrame({
            'age': np.random.randint(18, 60, 1000),
            'education_level': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], 1000),
            'career_goal': np.random.choice(['Data Science', 'Web Development'], 1000),
            'preferred_learning_style': np.random.choice(['Visual', 'Auditory', 'Reading', 'Kinesthetic'], 1000),
            'time_availability_hours_per_week': np.random.randint(5, 40, 1000),
            'learning_pace': np.random.uniform(0.5, 1.5, 1000),
            'weekly_study_hours': np.random.randint(5, 30, 1000),
            'platform_visits_per_week': np.random.randint(1, 20, 1000),
            'engagement_score': np.random.uniform(50, 100, 1000),
            'course_success_rate': np.random.uniform(60, 100, 1000),
            'course_history': [[] for _ in range(1000)]
        })

        # Add skill columns with appropriate distributions
        skills = ['python', 'statistics', 'machine_learning', 'html_css', 'javascript', 'react']
        for skill in skills:
            sample_data[f'skill_{skill}'] = np.random.uniform(0, 100, 1000)

        # Initialize and train recommender
        print("Initializing and training recommender...")
        recommender = CourseRecommender()
        recommender.train_model(sample_data)

        # Run test cases
        print("\nTesting with predefined cases...")
        test_cases = create_test_cases()
        test_recommender(recommender, test_cases)

    except Exception as e:
        print(f"An error occurred: {str(e)}")